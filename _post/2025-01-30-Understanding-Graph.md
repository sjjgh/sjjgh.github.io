# Graph basics
After I started working more closely with machine learning, I read and reviewed many papers related to graphs.

Many of them felt a bit odd to me—not because they were wrong, but because they often focused on inventing increasingly sophisticated tricks on graphs, without clearly explaining the intuition behind them.

Coming from a mathematics and physics background, I tend to care more about the reasoning behind a phenomenon than about engineering a black-box method. To me, good intuition usually comes from first asking a more fundamental question: what is a graph, and how should we interpret it in the context of machine learning?

During the first several years of my PhD, I spent a lot of time playing with graphs and embedding methods. Along the way, I gradually formed some perspectives that I rarely saw discussed explicitly. I felt they were worth writing down—and this blog is an attempt to do exactly that.

##What is graph
Graph, be definition is a set of nodes which is connected by edges. Every structure like this can be called graph like social networks where nodes are people and edges are their relationship, and brain network where each node is cell and edges is nuron connection. What make graph interesting is not beacuse they are universal, but also is beacuse they are a very efficent way to represetn relationship between entities. The relationshop might be control by mutliply factor: friendship might be forms beacuse two guy have similar hoobies, or in a same class, or live close. Brain cell connection are more likely control by rules encoded in genes.

In reality or machine learning, we are often gived a graph structure possibly with some node feature. Lets take a common and easy setups. Social network where edges means they are friends, and some features for each people such as age, gender, job, etc.
Now what can we say about the graph? First we need to have some agreement: 1. The edge is generated by certain rules and the rules are hidden, we will never know those rules. and most of time, we are not trying to find out the rule. For example, in the social networks, the hidden rule might be: people with similar hobby + different job + outer and inner personality+ both like outreach + age diff with in 4 has more probablity to become  friends. the rules could be more complicate along with probality instead of deterministic.
if our task is to predict where a person has a cat. Then we dont care about the rules. But we must understand, the hidden rule could be complex and in most case it would be impossible to revocer, and most of time, we care more feature than the rules. Second, we could dig some features from the graph, with some assumption. The more common and resonable assumption is the assumption of homophily, which is hiddenly used in almost every embeddig method. The assumption is the nodes with similar features are more tend to be connected. For example, the social networks, people with similar charatristic are more like to be friend is a reasonable simple assumption. By this assumption, we are able to create a vectors to each nodes such that the connected nodes are more close in vector space, as in geometry speaking, vectors close can be say more similar. This process is called embedding. The vector we can use as features, does not stand for any semantic features, it can be view as an abstract thing generated by our assumption, in this case, it can be viewed as a measure of similarity induced from graph. So the second agreement is: the feature we got is subject by assuption. And the information in this induced abstract feature, can be covered with observed feature, and can revail new features.
